---
title: 'Generalized Linear Model using R  '
author: dondon
date: '2021-02-12'
slug: generalized-linear-model-using-r
categories:
  - R
tags:
  - regression
output:
  blogdown::html_page:
      number_sections: true
      toc: TRUE
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#glm-소개"><span class="toc-section-number">0.1</span> glm 소개</a></li>
<li><a href="#glm-함수의-장단점"><span class="toc-section-number">0.2</span> glm 함수의 장단점</a></li>
<li><a href="#glmnet-소개"><span class="toc-section-number">0.3</span> glmnet 소개</a></li>
<li><a href="#glmnet의-장단점"><span class="toc-section-number">0.4</span> glmnet의 장단점</a></li>
<li><a href="#h2o-소개"><span class="toc-section-number">0.5</span> h2o 소개</a></li>
<li><a href="#h2o의-장단점"><span class="toc-section-number">0.6</span> h2o의 장단점</a>
<ul>
<li><a href="#section"><span class="toc-section-number">0.6.1</span> </a></li>
</ul></li>
<li><a href="#glm-관련-다른-패키지"><span class="toc-section-number">0.7</span> GLM 관련 다른 패키지</a></li>
</ul>
</div>

<pre class="r"><code>library(data.table)
library(tidyverse)
library(data.table)
library(dplyr)
library(MASS)
library(pscl)
library(MASS)</code></pre>
<p>서울시 bike sharing system 관련 분석 프로젝트를 하면서 glm 모형을 이용해서 모델링을 해볼 기회가 있었다. R에는 다양한 glm 관련 패키지가 있는데 각 패키지 별로 장단점이 존재한다. 이 글에서는 프로젝트를 하면서 사용해봤던 glm 패키지의 사용 방법에 대해 정리하고, 각 패키지의 장단점에 대해 기술해보겠다.</p>
<div id="glm-소개" class="section level2" number="0.1">
<h2><span class="header-section-number">0.1</span> glm 소개</h2>
<p>glm은 R에 기본적으로 내장되어있는 함수로 가장 많이 사용된다. 패키지 별로 glm을 어떻게 계산하는지 또는 다른 세팅을 어떻게 가져가는지에 따라 계산 결과가 달라질 수 있다. 따라서 glm 함수를 이용한 결과를 다른 패키지의 계산 결과의 신뢰성을 판단하는 척도로 사용했다. 사용 방법은 기본 lm 함수와 거의 비슷하다. family = link function만 지정해주면 generalized linear model을 피팅해준다. offset, weight 등을 설정하는 방법도 포함되어 있는데 거의 사용할 일은 없는 것 같다.</p>
<p><strong>Link function</strong></p>
<ul>
<li><p>binomial(link = “logit”)</p></li>
<li><p>gaussian(link = “identity”)</p></li>
<li><p>Gamma(link = “inverse”)</p></li>
<li><p>inverse.gaussian(link = “1/mu^2”)</p></li>
<li><p>poisson(link = “log”)</p></li>
<li><p>quasi(link = “identity”, variance = “constant”)</p></li>
<li><p>quasibinomial(link = “logit”)</p></li>
<li><p>quasipoisson(link = “log”)</p></li>
</ul>
<pre class="r"><code>p &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/data/poisson_sim.csv&quot;)
p &lt;- within(p, {
  prog &lt;- factor(prog, levels=1:3, labels=c(&quot;General&quot;, &quot;Academic&quot;, 
                                                     &quot;Vocational&quot;))
  id &lt;- factor(id)
})


fit1 &lt;- glm(num_awards ~ prog + math, family=&quot;poisson&quot;, data=p)
coef(fit1)</code></pre>
<pre><code>##    (Intercept)   progAcademic progVocational           math 
##     -5.2471244      1.0838591      0.3698092      0.0701524</code></pre>
<pre class="r"><code>summary(fit1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = num_awards ~ prog + math, family = &quot;poisson&quot;, data = p)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2043  -0.8436  -0.5106   0.2558   2.6796  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -5.24712    0.65845  -7.969 1.60e-15 ***
## progAcademic    1.08386    0.35825   3.025  0.00248 ** 
## progVocational  0.36981    0.44107   0.838  0.40179    
## math            0.07015    0.01060   6.619 3.63e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 287.67  on 199  degrees of freedom
## Residual deviance: 189.45  on 196  degrees of freedom
## AIC: 373.5
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="glm-함수의-장단점" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> glm 함수의 장단점</h2>
<p>R 기본함수로 glm이 잘 세팅되어 있는데 왜 다른 함수가 필요한지 의문일 수 있다. 솔직히 대학교, 대학원 범주형 자료분석 수업을 들을 때만 해도 glm 함수를 이용해서 모델 피팅을 했지 다른 패키지는 찾아볼 필요도 없었다. 그만큼 기본적인 것들이 대부분 세팅되어 있고, 관련 참고 자료나 분석 예시도 대부분 glm 함수를 이용한 것들이었다. 하지만 이번 프로젝트를 하면서 느낀 glm 함수의 치명적인 단점은 <strong>계산 속도</strong> 이다. 가령 설명 변수로 범주를 500개의 level로 구성된 factor를 넣었을 때(일반적으로 이런 일은 별로 없지만) 어떤 모델을 돌릴지에 따라 차이는 있지만 기본적으로 1시간 이상이 소요되었다. 또 <strong>sparse matrix</strong>를 이용한 모델 피팅이나 <strong>penalty term</strong>을 추가하는 등의 모델 피팅을 할 때 어려움이 있다.</p>
</div>
<div id="glmnet-소개" class="section level2" number="0.3">
<h2><span class="header-section-number">0.3</span> glmnet 소개</h2>
<p>glmnet 패키지는 stanford의 통계학과에서 만든 패키지이다. Trevor Hastie, Jerome Friedman, Rob Tibshirani 등 일반화 가법모형, LASSO 등을 개발한 유명한 통계학 교수들이 패키지를 개발하고 공동 관리를 하고 있다. 2021년까지 R 공식문서가 주기적으로 업데이트되고 있다.</p>
<p>glmnet은 glm과 달리 L1, L2 penalty term을 이용해서 모델 피팅을 한다. 따라서 penalty term의 형태, lambda를 어떻게 설정하는지에 따라 모델 피팅이 달라진다.</p>
<p>glmnet 패키지의 glmnet 함수는 glm함수와 link function을 지정해주는 방식은 거의 동일하다.</p>
<p><strong>Link function</strong></p>
<ul>
<li><p>binomial</p></li>
<li><p>gaussian</p></li>
<li><p>multinomial</p></li>
<li><p>poisson</p></li>
<li><p>cox</p></li>
<li><p>mgaussian</p></li>
<li><p>negative.binomial(theta)</p></li>
</ul>
<p>glmnet 함수는 glm과 달리 X에 대한 설계행렬을 만들어서 넣어주어야한다. 설계행렬은 model.matrix 함수를 이용해서 쉽게 만들 수 있다.</p>
<p>lambda = 0으로 지정할 경우 penalty term이 없는 glm의 결과와 비슷하다. parameter를 계산하는 방법이 glm함수와 다르기 때문에 coefficient 값이 완전히 같지는 않다. thresh 값을 아주 작게 지정해주면 coefficient 값이 거의 같아진다.</p>
<pre class="r"><code>library(glmnet)</code></pre>
<pre><code>## 필요한 패키지를 로딩중입니다: Matrix</code></pre>
<pre><code>## 
## 다음의 패키지를 부착합니다: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 4.1</code></pre>
<pre class="r"><code>data = model.matrix(num_awards ~ prog + math, data=p)
fit2 = glmnet(x = data, y = p$num_awards, family = &#39;poisson&#39;, lambda = 0, thresh = 1e-14)


coef(fit2)</code></pre>
<pre><code>## 5 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s0
## (Intercept)    -5.2471242
## (Intercept)     .        
## progAcademic    1.0838590
## progVocational  0.3698091
## math            0.0701524</code></pre>
<pre class="r"><code>coef(fit1)</code></pre>
<pre><code>##    (Intercept)   progAcademic progVocational           math 
##     -5.2471244      1.0838591      0.3698092      0.0701524</code></pre>
<p>glm에 penalty term을 추가해서 적합할 경우에 penalty term의 영향력을 조율하는 <span class="math inline">\(\lambda\)</span> 값을 지정해주어야 하는데 glmnet에서는 cross validation을 통해서 최적의 lambda 값을 찾는다. cv.glmnet 함수를 이용해서 mean cross validated error를 최소화하는 최적의 <span class="math inline">\(\lambda\)</span>값을 찾을 수 있다.</p>
<pre class="r"><code>cvfit2 = cv.glmnet(data, p$num_awards, family = &#39;poisson&#39;)
plot(cvfit2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>cvfit2$lambda.min</code></pre>
<pre><code>## [1] 0.02207919</code></pre>
<pre class="r"><code>coef(cvfit2, s = &quot;lambda.min&quot;)</code></pre>
<pre><code>## 5 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                          1
## (Intercept)    -4.80947229
## (Intercept)     .         
## progAcademic    0.79194737
## progVocational  .         
## math            0.06744891</code></pre>
</div>
<div id="glmnet의-장단점" class="section level2" number="0.4">
<h2><span class="header-section-number">0.4</span> glmnet의 장단점</h2>
<p>glmnet의 장점은 첫 번째로, <strong>계산 속도</strong>이다. base 함수인 glm에 비해서 월등히 빠르다. 가령 설명 변수로 범주를 500개의 level로 구성된 factor를 넣었을 때(일반적으로 이런 일은 별로 없지만) 3분 안에 계산이 되었다. 두 번째로, sparse matrix를 이용할 수 있다. data.matrix에 0의 비율이 많을 경우에 행렬 계산 속도가 현저하게 느려진다. 이를 해결하기 위해서 sparse matrix를 이용한 계산을 수행할 수 있는데 glmnet에서는 model matrix를 sparse matrix 형태로 바꾸기만 하면 이전과 동일하게 적용할 수 있다.</p>
<pre class="r"><code>data(&quot;SparseExample&quot;)
x</code></pre>
<pre><code>## 100 x 20 sparse Matrix of class &quot;dgCMatrix&quot;
##                                                                    
##   [1,]  .           .           0.7700000  .             .         
##   [2,]  .           .           0.7700000  .             2.28077765
##   [3,]  .           .           0.7700000  .             0.44767817
##   [4,]  .           0.86999603  0.7700000  .             .         
##   [5,]  .           .           0.7700000  .             .         
##   [6,]  .           .           0.7700000  0.3074177710 -0.61636863
##   [7,]  .           .           .          .             .         
##   [8,]  .           .           .          .             .         
##   [9,]  .           .           .          .             .         
##  [10,]  .           .           .          .             .         
##  [11,]  .           .           .          .             .         
##  [12,]  .           .           0.9177787  .             0.72764148
##  [13,]  .           .           .          .             .         
##  [14,]  .           .           .          .             .         
##  [15,]  .           .           .          .             0.09174319
##  [16,]  .           .           0.7700000  .             .         
##  [17,] -0.42002860  .           .          .             .         
##  [18,] -0.78794405  .           .          .             .         
##  [19,] -0.37957502  .           .          .             .         
##  [20,] -0.59042570  .          -0.2890381  .             .         
##  [21,]  .           0.76268971  .         -0.3287848641  .         
##  [22,]  .           .           .          .             .         
##  [23,]  .           .           1.4093311  .             .         
##  [24,]  .           .           .          0.7976562291  .         
##  [25,]  .           .           .         -0.0003429201  .         
##  [26,]  .           .           0.9906910  .             .         
##  [27,]  .           .           .          .             .         
##  [28,]  .           1.58616891  .         -1.6940213754  .         
##  [29,]  .           .           .          .             .         
##  [30,]  .           .           .          .             .         
##  [31,]  .           .           .          .             .         
##  [32,]  .           .           .          .             0.14773685
##  [33,]  .           .           .          .             .         
##  [34,]  .           .           .          .             .         
##  [35,]  .           .           .         -0.4216679429  .         
##  [36,]  .           .          -0.7339601  .             .         
##  [37,]  .           0.02003792  .         -0.2583120391  0.40305538
##  [38,]  .           .           .          .             .         
##  [39,]  .           .           0.7700000  .             .         
##  [40,]  .           .           .          .             .         
##  [41,]  .           .           .          .             .         
##  [42,]  .           .          -1.7901706  .             .         
##  [43,] -0.46637323 -0.36297500  .          .             .         
##  [44,]  .           .           .          .             .         
##  [45,]  .           .           .          .             0.85403287
##  [46,]  .           .           .         -1.7720229912  .         
##  [47,]  .           .           .          .             .         
##  [48,]  .           .           .         -0.6096882442  .         
##  [49,]  .           .           .          .             .         
##  [50,]  .           .           .          .             1.91317610
##  [51,]  .           .           .          .             0.76024898
##  [52,]  .           .           .          0.1861215793  .         
##  [53,]  .           .           .          .             .         
##  [54,] -0.10234648 -0.25331890  .          .             .         
##  [55,]  .           .          -0.5231977  .             1.55381264
##  [56,]  .          -0.79257014  .          .             .         
##  [57,] -0.14946445  .           0.6294910  .             .         
##  [58,]  .          -0.71409778  .          .             .         
##  [59,]  1.17391897  .           .          .             .         
##  [60,]  .           .           .          .             .         
##  [61,]  .           .           .          .             .         
##  [62,]  .          -0.98950960  .          .             0.11822334
##  [63,]  .           .           .          .             .         
##  [64,]  .           .           .          .             .         
##  [65,]  .           .           .          .             .         
##  [66,]  .           .           .          .            -0.15835670
##  [67,]  .           .          -0.5611989  .             .         
##  [68,]  .           .           0.6542807  .            -1.31481010
##  [69,]  .           .           .          .             .         
##  [70,]  .           .           .          .             .         
##  [71,]  .           .           .          .             .         
##  [72,]  .           0.16793019  .          .             .         
##  [73,]  .           .           .          .             .         
##  [74,]  0.51634269  .           .         -0.1543193877  .         
##  [75,]  .           0.48883377  .          .             .         
##  [76,]  .           .           .          .             .         
##  [77,]  .           .           .          .             .         
##  [78,]  .           .           .          .             .         
##  [79,]  1.12697045  0.05611637  .          .             .         
##  [80,]  .           0.62270624 -2.3866177  .             .         
##  [81,]  .           .           .          .             .         
##  [82,] -0.56304784  .           .          .             .         
##  [83,]  .           .           .          .             0.69382586
##  [84,]  .           .           .          .             .         
##  [85,]  .          -0.93592119  .          .             .         
##  [86,]  .           .           .          .             .         
##  [87,] -0.07270996  .           .          .             .         
##  [88,]  .           .           .          .             .         
##  [89,]  .           .          -0.3207713  .             .         
##  [90,]  .           .           .          .             .         
##  [91,]  .           .           .          .             .         
##  [92,] -0.32631238  .           .         -0.8025495158  .         
##  [93,]  .           0.80290965  1.0913215  .             .         
##  [94,]  0.57548489  .           .          1.0625129019  .         
##  [95,]  .           .           .          .             .         
##  [96,] -1.63939358  .           .         -0.7524842458  .         
##  [97,]  .           .           .          .             .         
##  [98,]  .           .          -0.8188306  .             .         
##  [99,]  .           .           .          .             .         
## [100,]  .           .           .          0.2412569161  .         
##                                                                            
##   [1,]  .            0.7700000  .          .          .           .        
##   [2,]  .            0.7700000  .          .          .           .        
##   [3,]  0.488209706  0.7700000  .          .          .           .        
##   [4,] -0.691528163  0.7700000  .          .          .           .        
##   [5,]  .            0.7700000  .          .          .           .        
##   [6,] -0.348230115  0.7700000  .          .          .           .        
##   [7,]  .            .          .          .         -0.55709582  .        
##   [8,]  0.247576620  .          .          .          .           .        
##   [9,]  .            .          .          .          .          -1.1029296
##  [10,] -0.560131978  .          .          .          .           0.3412264
##  [11,]  .            .         -0.1645140 -0.8846288  .           .        
##  [12,]  .            .          .          .          .           1.1034032
##  [13,]  .            .          .          .          .           .        
##  [14,]  0.943429728  .          .          .          .           .        
##  [15,]  .           -0.9174665  .          .          .          -0.2426110
##  [16,]  .            0.7700000  .          .          .           .        
##  [17,]  0.976744678 -0.3396986  0.4188062  .          .           .        
##  [18,]  .            .          .          .          .           .        
##  [19,] -0.941848019  0.6766927 -0.1394882  .          .           .        
##  [20,]  .            .          .          .          .           .        
##  [21,]  .            .          .          .          .           .        
##  [22,]  .            .          .         -1.6180640 -1.30150052  .        
##  [23,]  .            .          .          .          .           .        
##  [24,] -0.761866174  .          .          .          .           .        
##  [25,]  .            .          .          .          .           .        
##  [26,]  .            .          .          .          .           .        
##  [27,]  1.118712525  .          .          .          .           .        
##  [28,]  .            .          .          .          .           .        
##  [29,]  .            .          0.2903743  .          .           .        
##  [30,]  .            .          .          .         -0.20794293  .        
##  [31,]  .            .          .         -0.9919798  0.27305729  .        
##  [32,]  .            .          .          .          0.85807139  .        
##  [33,]  .            .          .          .          .          -0.9706153
##  [34,]  .            .          0.2858389  .         -1.18932194  0.8724007
##  [35,]  .            .          .         -0.5686972  .           .        
##  [36,]  .            .          .         -1.9129382  .           0.2399644
##  [37,]  .            1.1869186  .         -1.9827355  .          -1.5094037
##  [38,]  .            .          .          .          0.27317186  .        
##  [39,]  .            0.7700000  .          .          .           .        
##  [40,]  .            .          .          .          .           .        
##  [41,]  .            .          .          .          .           .        
##  [42,]  .            .          .          .          .           .        
##  [43,]  .            .          1.1175900  .          1.33384669  .        
##  [44,]  .            .          .          .          .           .        
##  [45,]  .            .          .          .          .           .        
##  [46,]  .            .          .          .          .           .        
##  [47,]  .            .          .          .          .           1.1877954
##  [48,]  .            .          .          .          .           .        
##  [49,] -0.001109353  .          .          .          .           .        
##  [50,]  .            .          .          1.0611657  .           .        
##  [51,]  .            .          .          .          .           .        
##  [52,] -0.937217446  .          .          .          .          -1.4438297
##  [53,]  .           -1.5011458  .          .          .           .        
##  [54,]  .            .         -1.8138497  .          .           .        
##  [55,]  .            .          .          .          .           1.5947085
##  [56,]  .            .          .          .          .           .        
##  [57,]  .            .         -1.0591109  .          .           .        
##  [58,]  .            2.1346086  .          .          .           .        
##  [59,]  .            0.9108024  .         -1.2787606  .           .        
##  [60,]  .            .          .          .          .           .        
##  [61,]  .            .          .          .          .           .        
##  [62,]  .            .          .          .          .           .        
##  [63,]  .            0.1554775  0.3580447  .          .           1.3185990
##  [64,]  .            .          .          .          .           .        
##  [65,]  .           -0.3953323  .         -2.4718081  .           .        
##  [66,]  .            .          .          .          0.61710283  .        
##  [67,]  .            .          .         -1.7336798  .           0.7841729
##  [68,]  .            .          .          .          .           .        
##  [69,]  .            .         -0.6127845  .          .           .        
##  [70,]  .           -0.7098837  .          .          .           .        
##  [71,]  .            .          .         -0.6810243  .           .        
##  [72,]  .            .         -1.1284473  .          .           .        
##  [73,]  .            .          .          .          .           .        
##  [74,]  .            .          .          .          .           .        
##  [75,]  .            .          .          0.3053392  .           .        
##  [76,]  .            .          .          .          0.01823328  .        
##  [77,]  .            .          .          .         -0.29201714  .        
##  [78,] -1.522528121  .          .          .          .           .        
##  [79,]  .            .          .          .          .           .        
##  [80,]  .            .          .          .          .           .        
##  [81,]  .            .          .          .          .           .        
##  [82,]  .            .          0.2954747  .          .           .        
##  [83,]  .            .          .         -0.5559146 -0.42254834  .        
##  [84,]  .            .          1.8254675  .          .           .        
##  [85,]  .            .          .          .          .           .        
##  [86,] -0.863461280  .          .          .          .           .        
##  [87,]  0.023612056  .          .         -0.5719587  .           .        
##  [88,]  .            1.1019473  0.5610575  .          .           .        
##  [89,]  .            .          .          .          .           .        
##  [90,]  .            0.1749733  .          .         -0.07162464  .        
##  [91,]  .            .          .          .          .           0.6131639
##  [92,]  .           -1.0655289  .          .          .           0.4672506
##  [93,]  .            .          1.5981686  0.5996707  .           .        
##  [94,]  .            .          .          .          .           .        
##  [95,]  .            0.3448876  .          .          .           .        
##  [96,]  .            1.1669018  .          .          .           .        
##  [97,]  .            .          .          .          .           .        
##  [98,]  .            .          .          .         -0.50391641  .        
##  [99,]  .            .          .          .          0.38464337  .        
## [100,]  .            .          .          .          .           .        
##                                                                              
##   [1,]  .          .           .          0.32968300  .           .          
##   [2,] -0.8980154  .           .          .           .           .          
##   [3,]  .          .           .          .           .           .          
##   [4,]  0.4761861  .           .          .           .           .          
##   [5,] -0.9897596 -1.40638658  .          .           .           .          
##   [6,]  .          .           .          .           .           .          
##   [7,]  .          0.09009345  .          .           .           0.001420549
##   [8,]  .          .           .          .          -0.86227686  .          
##   [9,]  .          .           .          .           .           .          
##  [10,]  .          .          -0.9114435  .           .           .          
##  [11,]  .          .           .          .           .           .          
##  [12,]  .          .           .          .          -1.66270756  .          
##  [13,]  .          .           .          0.68715719  .           .          
##  [14,]  .          .           .          .           .           .          
##  [15,]  .          .           .          .           .           .          
##  [16,]  .          .           .          .           .           .          
##  [17,] -1.4598310  0.24520605  .          .           .           .          
##  [18,]  .         -0.20067123  .          0.47422651 -0.83097174  0.232114203
##  [19,]  .          .           .          0.38573517  0.52055493 -0.834423547
##  [20,]  .          .           .          .           2.08318398  .          
##  [21,]  1.3362018  .           .         -0.29787213  .           .          
##  [22,]  .          .           .          .           .           .          
##  [23,]  .          .           .          .           .           .          
##  [24,] -2.1008817  .           .          .           .           .          
##  [25,]  .          0.85191676  .          .           .           .          
##  [26,]  .          .           .          .           1.48325311  .          
##  [27,]  .          .          -0.1109144  0.15807680  .          -0.030417597
##  [28,]  .          .           .          .           .           .          
##  [29,]  .          .           .          .           .           0.051457373
##  [30,] -0.3731898  .           .          .           0.06631204  0.192850613
##  [31,]  .          .           .          .           .           .          
##  [32,]  .          .           .          .           .           .          
##  [33,]  .          2.43457529  .          .           .           .          
##  [34,]  .          .           .          .           .           .          
##  [35,]  .          .           .          .           1.71663672  .          
##  [36,]  .          .           1.1517017  .          -0.79299099  .          
##  [37,]  .          .           .          .           .          -0.184218682
##  [38,]  .          .           .          .           .           .          
##  [39,]  .          .           .          .           .           .          
##  [40,]  .          .          -0.2602759  .           .           .          
##  [41,]  .          .           .          .           .           1.531880342
##  [42,]  .         -0.96849073  .          .           .           .          
##  [43,]  .          .           .          .           .           .          
##  [44,]  1.7985498  .           .          .           .           .          
##  [45,]  .          .           .          .          -0.20889564  .          
##  [46,]  .          .           0.9356432  .           .           .          
##  [47,]  1.1478612  .           .          .           .           .          
##  [48,]  .         -0.42885917  .          .           .           .          
##  [49,]  .          .           .         -0.29226518  .           0.995782603
##  [50,]  .          .           .          .           .           .          
##  [51,]  .          .           .          .           .           0.107879213
##  [52,]  .          .           .          .           .           .          
##  [53,]  .          .           .          .           .           .          
##  [54,]  .          0.24113704 -2.5553898  .           .           .          
##  [55,]  .          .           .          .           .           .          
##  [56,]  .          .           .          .           .           .          
##  [57,]  .          .           .          .           .           .          
##  [58,] -1.2012123  .           .          .           .           .          
##  [59,]  .          0.31550432  .          .           .           .          
##  [60,]  .          .           0.1331203  .           .           .          
##  [61,]  .          0.07069286 -0.1436248  0.11421565  .           .          
##  [62,]  .          .          -1.3981768  .           .           .          
##  [63,] -1.1838296  .           .          .           .           .          
##  [64,]  .          .           .          0.06267201  .           .          
##  [65,]  .          .           .          .           .          -0.317718749
##  [66,]  .          .          -0.5633044  .           .           .          
##  [67,]  .          .           .          .           .           .          
##  [68,]  .          .           1.1046609  .           0.46918449  .          
##  [69,]  1.9147380  .           .          .           .           .          
##  [70,]  .          .           .          .           .           .          
##  [71,]  .          .           0.5438430  .           .           .          
##  [72,]  .          .           .          .           .           .          
##  [73,]  .          .           .          .           .           .          
##  [74,]  0.3428178 -0.89779529  .          0.25104721  0.24796073  .          
##  [75,]  .          1.17821058  .          .           .           1.359770613
##  [76,]  .          .           .          .           .           .          
##  [77,]  .          .          -1.0284542  0.49597513  .           .          
##  [78,]  .          .           .          0.67945431  .           .          
##  [79,]  .          .           .          .           .           .          
##  [80,]  .          .          -0.4787670  .           .           .          
##  [81,]  .          .           .          .           .           .          
##  [82,]  .          .           .          .           .           .          
##  [83,]  .          .           .          .           .           .          
##  [84,]  .          0.60367444  .         -2.01130819  .           .          
##  [85,]  .         -0.14592223  .          .           .           .          
##  [86,]  .          .           .          .           .           1.061393446
##  [87,]  .          .           .          .           .           .          
##  [88,]  .          .           0.1087806  .           .           .          
##  [89,]  0.4183668  .           .          .           .           .          
##  [90,]  .          .           .          .           .           .          
##  [91,]  .          .           .          .          -1.42980520  .          
##  [92,]  .          .           .          .           .           .          
##  [93,]  .          .           .          .           .           .          
##  [94,]  .          .           .          .           .           .          
##  [95,]  0.8310154  .           .          .           .           .          
##  [96,]  .          .           .          .           .          -1.754500268
##  [97,]  .          .           .         -0.78600201  .          -0.684068370
##  [98,]  .          .           .          .           0.82440597  .          
##  [99,]  .          .           .          .           0.27991720  .          
## [100,]  .          .           .          1.07624006  .           .          
##                                          
##   [1,]  .           .           0.7700000
##   [2,]  .           .           0.7700000
##   [3,]  .          -1.19908077  0.7700000
##   [4,]  .           .           0.7700000
##   [5,]  0.77013657  .           0.7700000
##   [6,]  .           .           0.7700000
##   [7,]  .           .           .        
##   [8,] -0.74941085  .           .        
##   [9,]  1.16753556  .           .        
##  [10,]  0.38150158 -0.46859672  .        
##  [11,]  .           .           .        
##  [12,]  .           1.05198884  .        
##  [13,]  .           .           .        
##  [14,]  .           .           .        
##  [15,]  .           .           .        
##  [16,]  .           .           0.7700000
##  [17,]  .           .           0.3931508
##  [18,]  .          -0.80551294  .        
##  [19,]  .           .           .        
##  [20,]  .           .          -1.3076536
##  [21,]  .           .           .        
##  [22,]  .           .           .        
##  [23,]  .           .           .        
##  [24,]  .          -0.75928691  .        
##  [25,]  .           .           .        
##  [26,]  .           .           .        
##  [27,]  .           .           0.5513249
##  [28,]  .           .           .        
##  [29,]  .          -2.20663929  .        
##  [30,]  .           .           .        
##  [31,]  .           .           .        
##  [32,]  .           .           .        
##  [33,]  .           .           .        
##  [34,] -0.41481022  .           .        
##  [35,]  1.93485976  .          -2.2588707
##  [36,]  .           .          -1.6705721
##  [37,]  .           .           .        
##  [38,]  .           .           .        
##  [39,]  .           .           0.7700000
##  [40,]  .           1.94049094  .        
##  [41,]  .           .          -0.3685950
##  [42,]  .           .           .        
##  [43,]  .           .           .        
##  [44,]  .           .           .        
##  [45,]  .           .           .        
##  [46,]  .           .           .        
##  [47,]  .           0.69896753  .        
##  [48,]  .           .           .        
##  [49,]  .           .           .        
##  [50,]  .           .           .        
##  [51,]  .           .           .        
##  [52,]  .           .           .        
##  [53,]  .           .           .        
##  [54,]  .           0.51077638  .        
##  [55,]  .           1.00637478  .        
##  [56,]  .           .           .        
##  [57,]  0.68172235  .           .        
##  [58,]  .           .           0.5793416
##  [59,]  .           .           .        
##  [60,]  .           .           .        
##  [61,]  .           .           .        
##  [62,]  .           .           0.5885315
##  [63,]  .           .           .        
##  [64,]  .           .           .        
##  [65,]  0.90209926  0.76338040  .        
##  [66,]  .           .           .        
##  [67,]  .           .           .        
##  [68,]  .           0.63405505  .        
##  [69,]  1.33748417  .           .        
##  [70,]  .           .          -1.5223148
##  [71,]  .           .           .        
##  [72,]  .           .           .        
##  [73,]  2.31110937  .           .        
##  [74,]  .           .           .        
##  [75,]  .           .           .        
##  [76,]  .           .           .        
##  [77,]  .           1.06914808  0.4667950
##  [78,]  0.92418230  .           0.6049404
##  [79,]  .           .           .        
##  [80,]  0.06539568  .           .        
##  [81,] -0.94828872  .           .        
##  [82,]  .           .           .        
##  [83,] -0.09322383  .           .        
##  [84,]  .           .           .        
##  [85,]  .           .          -0.1337481
##  [86,]  .           0.09100132  .        
##  [87,]  .           .           .        
##  [88,]  .           .           0.6826486
##  [89,]  2.83746602  .          -0.3884004
##  [90,]  .           .           .        
##  [91,]  .           .           .        
##  [92,]  .           .           .        
##  [93,]  .           .           .        
##  [94,]  .           .           .        
##  [95,]  .           .           .        
##  [96,]  .           .           .        
##  [97,]  .           .           .        
##  [98,]  .           .           .        
##  [99,]  .          -1.13106125  .        
## [100,]  .           .           .</code></pre>
<pre class="r"><code>fit_ex = glmnet(x, y)
cvfit_ex = cv.glmnet(x, y)
plot(cvfit_ex)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>coef(cvfit_ex, s = &quot;lambda.min&quot;)</code></pre>
<pre><code>## 21 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept) -0.02499004
## V1           .         
## V2           0.52332164
## V3           0.12959290
## V4          -0.21519038
## V5           0.62107665
## V6           0.80719780
## V7          -0.28912431
## V8          -0.20269905
## V9          -0.63856799
## V10          .         
## V11          0.03754506
## V12          0.22102168
## V13         -0.29326168
## V14          .         
## V15         -1.69869773
## V16          .         
## V17          .         
## V18         -0.13159375
## V19         -0.93302616
## V20          .</code></pre>
<pre class="r"><code># sparsedata &lt;- sparse.model.matrix(y~., data = data)
# glmnet(x = sparsedata, y = data$y, family = poisson)</code></pre>
<p>glmnet의 단점은 <strong>p-value</strong> 계산이 안된다는 것이다. 즉, coefficient에 대한 significance test를 할 수 없다. penalty term을 이용해서 parameter를 최적화하기 때문에 base 함수인 glm처럼 명시적으로 <strong>p-value</strong> 를 계산할 수 없으며, 따라서 coefficient에 대한 직관적인 해석이 어렵다.</p>
</div>
<div id="h2o-소개" class="section level2" number="0.5">
<h2><span class="header-section-number">0.5</span> h2o 소개</h2>
<p>h2o는 자바 기반의 machine learning/deep learning 플랫폼이다. glm 관련 패키지를 찾다가 발견했는데 생각보다 많은 모델을 돌릴 수 있고, 분석 뿐만 아니라 모델 배포까지 지원해주는 것 같다. 특정 회사에서 관리하고 있는 것 같은데 전세계적으로 이용하는 패키지이므로 믿고 써도 될 것 같다.</p>
<p>h2o를 사용하기 위해서는 java를 세팅해야 하는데 우선 코드를 돌려보고 error가 뜨면 java 몇 버전을 설치하라고 링크를 알려준다. 링크를 타고 들어가서 java만 버전에 맞게 설치하면 error 없이 동작한다. (혹시 다른 error가 발생할 경우 참고 6 링크)</p>
<p>**Link function**</p>
<ul>
<li><p><code>gaussian</code>: The data must be numeric (Real or Int). This is the default family.</p></li>
<li><p><code>binomial</code>: The data must be categorical 2 levels/classes or binary (Enum or Int).</p></li>
<li><p><code>multinomial</code>: The data can be categorical with more than two levels/classes (Enum).</p></li>
<li><p><code>ordinal</code>: Requires a categorical response with at least 3 levels. (For 2-class problems use family=“binomial”.)</p></li>
<li><p><code>quasibinomial</code>: The data must be numeric.</p></li>
<li><p><code>poisson</code>: The data must be numeric and non-negative (Int).</p></li>
<li><p><code>gamma</code>: The data must be numeric and continuous and positive (Real or Int).</p></li>
<li><p><code>tweedie</code>: The data must be numeric and continuous (Real) and non-negative.</p></li>
</ul>
<p>h2o를 사용하기 위해서는 h2o.init 함수를 이용해서 h2o와 연결해주어야 한다. 그리고 as.h2o 함수를 이용해서 h2o 패키지에서 이용할 수 있는 데이터 프레임으로 세팅해주어야 한다.</p>
<pre class="r"><code>library(h2o)
head(p)</code></pre>
<pre><code>##    id num_awards       prog math
## 1  45          0 Vocational   41
## 2 108          0    General   41
## 3  15          0 Vocational   44
## 4  67          0 Vocational   42
## 5 153          0 Vocational   40
## 6  51          0    General   42</code></pre>
<pre class="r"><code>h2o.init()</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 hours 26 seconds 
##     H2O cluster timezone:       Asia/Seoul 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.32.0.1 
##     H2O cluster version age:    4 months and 4 days !!! 
##     H2O cluster name:           H2O_started_from_R_uos_xcm908 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   1.97 GB 
##     H2O cluster total cores:    6 
##     H2O cluster allowed cores:  6 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4 
##     R Version:                  R version 4.0.3 (2020-10-10)</code></pre>
<pre class="r"><code>h2o_df &lt;- as.h2o(p)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%</code></pre>
<p>h2o dataframe을 세팅한 후에는 predictor와 response에 해당하는 <strong>변수명</strong>을 각각 할당해주어야 한다.</p>
<p>h2o.glm은 glm의 다양한 세팅을 지원하는데 glmnet과 유사하다.</p>
<p><strong>solver</strong></p>
<ul>
<li><p>IRLSM: Iteratively Reweighted Least Squares Method (default)</p></li>
<li><p>L_BFGS: Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm</p></li>
<li><p>AUTO: Sets the solver based on given data and parameters.</p></li>
<li><p>COORDINATE_DESCENT: Coordinate Decent (not available when <code>family=multinomial</code>)</p></li>
<li><p>COORDINATE_DESCENT_NAIVE: Coordinate Decent Naive</p></li>
<li><p>GRADIENT_DESCENT_LH: Gradient Descent Likelihood (available for Ordinal family only; default for Ordinal family)</p></li>
<li><p>GRADIENT_DESCENT_SQERR: Gradient Descent Squared Error (available for Ordinal family only)</p></li>
</ul>
<p><strong>IRLSM은 base 함수인 glm의 solver이다</strong>.</p>
<p><strong>coordinate descent는 glmnet의 solver이다.</strong></p>
<p>compute_p_value = T : lambda를 0으로 세팅할 경우에 P-value를 계산해준다.</p>
<p>remove_collinear_columns = T : colinear column을 자동으로 제거할지 여부. lambda = 0일 경우에만 세팅할 수 있다.</p>
<pre class="r"><code>predictors &lt;- colnames(p)[c(2,3)]
response &lt;- colnames(p)[4]
fit3 &lt;- h2o.glm(x = predictors, y = response, training_frame = h2o_df, 
                   family = &#39;poisson&#39;, link = &#39;log&#39;, solver = &#39;IRLSM&#39;, lambda = 0,
                   compute_p_values = T, remove_collinear_columns = T)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |======================================================================| 100%</code></pre>
<pre class="r"><code>h2o.performance(fit3)</code></pre>
<pre><code>## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  56.90876
## RMSE:  7.54379
## MAE:  6.252057
## RMSLE:  0.1414839
## Mean Residual Deviance :  1.077541
## R^2 :  0.3483404
## Null Deviance :330.0766
## Null D.o.F. :199
## Residual Deviance :215.5082
## Residual D.o.F. :196
## AIC :1381.296</code></pre>
<pre class="r"><code>fit3@model$coefficients_table</code></pre>
<pre><code>## Coefficients: glm coefficients
##             names coefficients std_error    z_value  p_value
## 1       Intercept     3.978105  0.016425 242.195658 0.000000
## 2    prog.General    -0.077400  0.026056  -2.970575 0.002972
## 3 prog.Vocational    -0.154611  0.025666  -6.023992 0.000000
## 4      num_awards     0.057477  0.009189   6.255012 0.000000
##   standardized_coefficients
## 1                  4.014315
## 2                 -0.077400
## 3                 -0.154611
## 4                  0.060519</code></pre>
</div>
<div id="h2o의-장단점" class="section level2" number="0.6">
<h2><span class="header-section-number">0.6</span> h2o의 장단점</h2>
<p>h2o의 장점은 첫 번째로, <strong>계산속도이다</strong>. sparse matrix를 이용한 glmnet과 거의 동일한 속도로 계산된다. 따라서 sparse matrix를 따로 만들지 않아도 된다는 장점이 있다. 두 번째로, 패키지의 확장성이다. h2o의 다른 개발관련 함수와 연계하면 모델 배포를 쉽게 할 수 있는 장점이 있을 것 같다. 세 번째로, 쉬운 함수 설정 방식이다. ML 패키지답게 parameter만 세팅하면 쉽게 계산할 수 있게 만든 것 같다.</p>
<p>h2o의 단점은 계산 결과에 대한 신뢰성이 부족하다. 치명적인 단점인 것 같다. 공식 문서에는 glm과 h2o.glm의 결과를 같게 하기 위해서는 다음과 같이 세팅하라고 권고하고 있다.</p>
<pre><code>solver = &quot;IRLSM&quot;
lambda = 0
remove_collinear_columns = TRUE
compute_p_values = TRUE</code></pre>
<p>하지만 parameter 값을 동일하게 세팅해봐도 coefficient, deviance, aic 값이 base glm과 전부 다르다. Example data에 대한 h2o.glm과 base glm의 coefficient를 보면 부호가 다르다. 부호가 다르면 해석이 달라지기 때문에 h2o.glm 결과의 신뢰성에 의문이 생길 수밖에 없는 것 같다.</p>
<div id="section" class="section level3" number="0.6.1">
<h3><span class="header-section-number">0.6.1</span> </h3>
</div>
</div>
<div id="glm-관련-다른-패키지" class="section level2" number="0.7">
<h2><span class="header-section-number">0.7</span> GLM 관련 다른 패키지</h2>
<ul>
<li><p>pscl : zero inflated regression 관련된 패키지로 R에서는 독보적이다.</p></li>
<li><p>MASS : glm.nb 함수를 이용해서 negative binomial regression을 피팅할 수 있다. negative binomial regression을 피팅할 때 theta 값을 지정해주어야 하는데 MASS 패키지에서는 theta 값을 지정해주지 않아도 패키지 내에서 최적의 theta 값을 찾아준다. (glmnet, h2o, base glm에서는 전부 theta 값을 지정해주어야한다)</p></li>
</ul>
<p>참고 1 : <a href="https://stackoverflow.com/questions/38378118/lasso-with-lambda-0-and-ols-produce-different-results-in-r-glmnet" class="uri">https://stackoverflow.com/questions/38378118/lasso-with-lambda-0-and-ols-produce-different-results-in-r-glmnet</a></p>
<p>참고 2 : <a href="https://cran.r-project.org/web/packages/glmnet/glmnet.pdf" class="uri">https://cran.r-project.org/web/packages/glmnet/glmnet.pdf</a></p>
<p>참고 3 : <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html" class="uri">https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></p>
<p>참고 4 : <a href="https://stats.stackexchange.com/questions/45449/when-using-glmnet-how-to-report-p-value-significance-to-claim-significance-of-pr" class="uri">https://stats.stackexchange.com/questions/45449/when-using-glmnet-how-to-report-p-value-significance-to-claim-significance-of-pr</a></p>
<p>참고 5 : <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html" class="uri">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html</a></p>
<p>참고 6 : <a href="https://rstudio-pubs-static.s3.amazonaws.com/359032_6d2fa1280f8a40a582c8a40fb46c8c15.html" class="uri">https://rstudio-pubs-static.s3.amazonaws.com/359032_6d2fa1280f8a40a582c8a40fb46c8c15.html</a></p>
</div>
